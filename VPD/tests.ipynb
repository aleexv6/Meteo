{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "from config import DATA_DOWNLOAD_URL\n",
    "from shapely.geometry import Point\n",
    "import geopandas\n",
    "from shapely.prepared import prep\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Dataset(f\"{DATA_DOWNLOAD_URL}raw/ERA5_VPD_at_Tmax_2000.nc4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4.Dataset'>\n",
       "root group (NETCDF4 data model, file format HDF5):\n",
       "    description: Original data from ERA5\n",
       "    creation_date: 20210125T114114Z\n",
       "    author: Carlo Montes, CIMMYT, c.montes@cgiar.org\n",
       "    dimensions(sizes): lon(1440), lat(601), doy(366)\n",
       "    variables(dimensions): float32 vpd(doy, lat, lon), float32 lon(lon), float32 lat(lat), float32 doy(doy)\n",
       "    groups: "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(601, 1440)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.variables[\"vpd\"][:][0].shape #file.variables[\"vpd\"][:][x] = year from 0 to 365 ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file.variables[\"vpd\"][:][x] is shape : \n",
    "#latitude[0][.. .. .. .. longitude from -180 to 180 .. .. .. ..]\n",
    "#latitude[1][.. .. .. .. longitude from -180 to 180 .. .. .. ..]\n",
    "#...\n",
    "#latitude[601][.. .. .. .. longitude from -180 to 180 .. .. .. ..]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flattened data (for each year) is : [data[lat0, lon0], data[lat0, lon1], data[lat0, lon2], ... data[lat1, lon0], data[lat1, lon1], ..., data[lat601, lon1440]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We make list of point and the index corresponding following the definition of our flattened data\n",
    "pointsDict = {}\n",
    "points = []\n",
    "compteur = 0\n",
    "for lat in file.variables[\"lat\"][:]:\n",
    "    for lon in file.variables[\"lon\"][:]:\n",
    "        pointsDict[Point(lon, lat)] = compteur #make a dict with points as index and future flattened data index as value\n",
    "        points.append(Point(lon, lat))\n",
    "        compteur += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = geopandas.read_file(\"data/geojsonfrance_corse_20.json\") #get polygon values for each french dep\n",
    "geo[\"code\"] = geo[\"code\"].astype(int)\n",
    "geo = geo.sort_values(by=\"code\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     mean_datas \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m---> 20\u001b[0m resList\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdep\u001b[39m\u001b[38;5;124m\"\u001b[39m: indexes_dep[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m: date\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvpd_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(max_data), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvpd_min\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(min_data), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvpd_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(mean_datas)})\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdep\u001b[39m\u001b[38;5;124m\"\u001b[39m: indexes_dep[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m: date\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvpd_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(max_data), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvpd_min\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(min_data), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvpd_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(mean_datas)})\n\u001b[0;32m     22\u001b[0m date \u001b[38;5;241m=\u001b[39m date \u001b[38;5;241m+\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_data' is not defined"
     ]
    }
   ],
   "source": [
    "for coords_file in os.listdir(\"data/coords/\"):\n",
    "    resList = []\n",
    "    with open(f\"data/coords/{coords_file}\") as f:\n",
    "        indexes_dep = json.load(f)\n",
    "    for vpd_file in os.listdir(f\"{DATA_DOWNLOAD_URL}raw/\"):\n",
    "        file = Dataset(f\"{DATA_DOWNLOAD_URL}raw/{vpd_file}\")\n",
    "        year = vpd_file.split(\"_\")[4].split(\".\")[0]\n",
    "        date = datetime(int(year), 1, 1)\n",
    "        data = file.variables[\"vpd\"][:].filled(np.nan)\n",
    "        for i in range(len(data)): # for each year in dataset\n",
    "            data_flatten = data[i].flatten()\n",
    "            if indexes_dep[\"valid_index\"]: #check if there is at least an index for this dep\n",
    "                datas = data_flatten[indexes_dep[\"valid_index\"]]\n",
    "                mean_datas = np.nanmean(datas) #ignore nan values\n",
    "                max_data = np.nanmax(datas)\n",
    "                min_data = np.nanmin(datas)\n",
    "            else:\n",
    "                mean_datas = np.nan\n",
    "                max_data = np.nan\n",
    "                min_data = np.nan\n",
    "            resList.append({\"dep\": indexes_dep[\"name\"], \"date\": date.strftime(\"%Y-%m-%d\"), \"vpd_max\": float(max_data), \"vpd_min\": float(min_data), \"vpd_mean\": float(mean_datas)})\n",
    "            date = date + timedelta(days=1)\n",
    "    with open(f\"{DATA_DOWNLOAD_URL}dailyDepDatas/{indexes_dep[\"name\"]}.json\", \"w\") as outfile:\n",
    "        outfile.write(json.dumps(resList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, dep in geo.iterrows():\n",
    "    prepared = prep(dep[\"geometry\"])\n",
    "    valid_points = []\n",
    "    valid_points.extend(filter(prepared.contains, pointsDict))\n",
    "    valid_indices = [pointsDict[point] for point in valid_points if point in points]\n",
    "    res = {\"name\": dep[\"nom\"], \"valid_index\": valid_indices}\n",
    "    with open(f\"data/coords/{dep[\"code\"]}-{dep[\"nom\"]}.json\", \"w\") as outfile: \n",
    "        json.dump(res, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
